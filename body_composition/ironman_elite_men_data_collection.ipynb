{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to collect some athletes from the internet - let's do a power analysis to determine the minimum amount of height and weights I would need - some assumptions are that there is a medium effect size between height and weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.92794663444455"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "# Parameters for the power analysis\n",
    "effect_size = 0.5  # medium effect size\n",
    "alpha = 0.05  # 95% confidence level\n",
    "power = 0.95  # 95% power\n",
    "\n",
    "# Create a TTestIndPower object\n",
    "analysis = TTestIndPower()\n",
    "\n",
    "# Calculate required sample size\n",
    "sample_size = analysis.solve_power(effect_size=effect_size, alpha=alpha, power=power, alternative='two-sided')\n",
    "sample_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "105 athletes is the goal - I found a rankings site that has the height and weight of multiple atheltes - Let's make sure I can scrape the website by pulling the robots.txt file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch the robots.txt file\n",
    "robots_url = 'https://stats.protriathletes.org/robots.txt'\n",
    "response = requests.get(robots_url)\n",
    "\n",
    "# Display the content of robots.txt\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is nothing specific in the robots.txt file about scraping - let's move forward but respect the servers by adding a delay between the requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "from tqdm.asyncio import tqdm\n",
    "import random\n",
    "\n",
    "# Add a delay function for respectful scraping\n",
    "async def delay():\n",
    "    await asyncio.sleep(random.uniform(1.5, 3))  # Sleep for 1.5 to 3 seconds between requests\n",
    "\n",
    "async def scrape_athlete_profile(page, url):\n",
    "    \"\"\"Scrape an individual athlete's profile for name, height, and weight.\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        print(f\"Fetching {url}\")\n",
    "\n",
    "        # Visit the athlete profile page\n",
    "        await page.goto(url)\n",
    "        \n",
    "        # Wait for the page to load the name, height, and weight information\n",
    "        await page.wait_for_selector(\"div.athlete-info\", timeout=60000)\n",
    "\n",
    "        # Extract athlete name\n",
    "        athlete_name = await page.text_content(\"h2.headline.font-weight-bold\")\n",
    "        \n",
    "        # Extract height and weight\n",
    "        height = await page.text_content(\"div.attribute:has-text('Height') .value\")\n",
    "        weight = await page.text_content(\"div.attribute:has-text('Weight') .value\")\n",
    "\n",
    "        # Store athlete data\n",
    "        athlete_data = {\n",
    "            \"name\": athlete_name.strip(),\n",
    "            \"height\": height.strip(),\n",
    "            \"weight\": weight.strip(),\n",
    "            \"url\": url\n",
    "        }\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Successfully scraped {athlete_name} in {elapsed_time:.2f} seconds\")\n",
    "        return athlete_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def scrape_all_athletes():\n",
    "    \"\"\"Scrape all athletes from the men's rankings page.\"\"\"\n",
    "    base_url = \"https://stats.protriathletes.org\"\n",
    "    athlete_urls = []\n",
    "    athletes_data = []\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        # Step 1: Scrape the athlete URLs from the rankings page\n",
    "        await page.goto(f\"{base_url}/rankings/men\")\n",
    "        await page.wait_for_selector('.rankings', timeout=60000)\n",
    "\n",
    "        # Extract athlete profile URLs\n",
    "        athlete_links = await page.query_selector_all('div.trow a.athlete-pic-group')\n",
    "        athlete_urls = [base_url + await link.get_attribute('href') for link in athlete_links]\n",
    "\n",
    "        print(f\"Found {len(athlete_urls)} athlete URLs.\")\n",
    "        \n",
    "        # Step 2: Scrape each athlete's profile\n",
    "        for url in tqdm(athlete_urls, desc=\"Scraping Athletes\", unit=\"profile\"):\n",
    "            athlete_data = await scrape_athlete_profile(page, url)\n",
    "            if athlete_data:\n",
    "                athletes_data.append(athlete_data)\n",
    "            await delay()  # Add delay between requests to respect the server\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    df = pd.DataFrame(athletes_data)\n",
    "    output_path = '/Users/ngirmay/Documents/GitHub/ironman_retrospective/IronMan_2023/body_composition/athlete_profiles.csv'\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"Scraping completed. Total athletes scraped: {len(athletes_data)}\")\n",
    "    print(f\"File saved successfully at: {output_path}\")\n",
    "\n",
    "# Run the scraping function\n",
    "async def main():\n",
    "    start_time = time.time()\n",
    "    await scrape_all_athletes()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Total scraping time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All available height and weights from the PTO rankings were aquired - let's combine with a dataset I found from the Rio 2016 Olympics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully at: /Users/ngirmay/Documents/GitHub/ironman_retrospective/IronMan_2023/body_composition/All_athletes_height_weight.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "file_rio_path = '/Users/ngirmay/Documents/GitHub/ironman_retrospective/IronMan_2023/body_composition/all-rio-2016-athletes-excel.csv'\n",
    "file_profiles_path = '/Users/ngirmay/Documents/GitHub/ironman_retrospective/IronMan_2023/body_composition/athlete_profiles.csv'\n",
    "output_combined_file_path = '/Users/ngirmay/Documents/GitHub/ironman_retrospective/IronMan_2023/body_composition/All_athletes_height_weight.csv'\n",
    "\n",
    "# Load the files\n",
    "file_rio_df = pd.read_csv(file_rio_path)\n",
    "file_profiles_df = pd.read_csv(file_profiles_path)\n",
    "\n",
    "# Function to clean height and weight columns\n",
    "def clean_height_weight(value):\n",
    "    if isinstance(value, str):\n",
    "        match = re.search(r'[\\d.]+', value)\n",
    "        if match:\n",
    "            return float(match.group())\n",
    "    return None\n",
    "\n",
    "# Clean and adjust height and weight for Rio 2016 dataset\n",
    "file_rio_df['height (m)'] = file_rio_df['height (m)'].apply(lambda x: x / 100 if x > 2.5 else x)  # Convert heights like 180 to 1.80\n",
    "file_rio_df['weight (kg)'] = file_rio_df['weight (kg)'].apply(lambda x: x / 10 if x > 300 else x)  # Convert weights like 158 to 75.8 kg\n",
    "\n",
    "# Filter male triathletes from Rio 2016 dataset\n",
    "file_rio_df_triathletes = file_rio_df[(file_rio_df['sport'] == 'triathlon') & (file_rio_df['sex'] == 'male')]\n",
    "\n",
    "# Clean and adjust height and weight for athlete profiles dataset\n",
    "file_profiles_df['Cleaned_Height_Profiles'] = file_profiles_df['height'].apply(clean_height_weight)\n",
    "file_profiles_df['Cleaned_Weight_Profiles'] = file_profiles_df['weight'].apply(clean_height_weight)\n",
    "\n",
    "file_profiles_df['Cleaned_Height_Profiles'] = file_profiles_df['Cleaned_Height_Profiles'].apply(lambda x: x / 100 if x > 2.5 else x)\n",
    "file_profiles_df['Cleaned_Weight_Profiles'] = file_profiles_df['Cleaned_Weight_Profiles'].apply(lambda x: x / 10 if x > 300 else x)\n",
    "\n",
    "# Ensure 'name' column is consistent across datasets\n",
    "if 'Name' in file_profiles_df.columns:\n",
    "    file_profiles_df.rename(columns={\"Name\": \"name\"}, inplace=True)\n",
    "\n",
    "# Merging the cleaned dataframes\n",
    "merged_df = pd.merge(\n",
    "    file_rio_df_triathletes[['name', 'sport', 'nationality', 'date_of_birth', 'height (m)', 'weight (kg)']],\n",
    "    file_profiles_df[['name', 'Cleaned_Height_Profiles', 'Cleaned_Weight_Profiles']],\n",
    "    on='name',\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Combine height and weight columns (use profiles data if available, then Rio data)\n",
    "merged_df['Final_Height'] = merged_df.apply(lambda row: row['Cleaned_Height_Profiles'] if pd.notna(row['Cleaned_Height_Profiles']) \n",
    "                                             else row['height (m)'], axis=1)\n",
    "merged_df['Final_Weight'] = merged_df.apply(lambda row: row['Cleaned_Weight_Profiles'] if pd.notna(row['Cleaned_Weight_Profiles']) \n",
    "                                             else row['weight (kg)'], axis=1)\n",
    "\n",
    "# Combine the source columns into one\n",
    "merged_df['Source'] = merged_df.apply(lambda row: 'PRO Triathletes' if pd.notna(row['Cleaned_Height_Profiles']) \n",
    "                                      else 'Rio 2016 Olympics', axis=1)\n",
    "\n",
    "# Drop extra columns\n",
    "merged_df.drop(columns=['height (m)', 'weight (kg)', 'Cleaned_Height_Profiles', 'Cleaned_Weight_Profiles'], inplace=True)\n",
    "\n",
    "# Save the final dataset to a CSV file\n",
    "merged_df.to_csv(output_combined_file_path, index=False)\n",
    "\n",
    "print(f\"File saved successfully at: {output_combined_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end I was able to collect 267 heights and 239 weights from elite male triathletes - this should be more than enough to complete the analysis "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
